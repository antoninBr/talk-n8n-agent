# ğŸ¤ Talk n8n Agent - IA locales ou distantes, outils, et un chef d'orchestre nommÃ© n8n

Comment faire bosser une IA avec vos outils du quotidien ? Dans cette confÃ©rence, on explore comment orchestrer des agents IA avec n8n, une plateforme dâ€™automatisation simple et visuelle. On y connecte des modÃ¨les IA (locaux avec Ollama, ou distant) et des outils grÃ¢ce au protocole MCP. Une approche concrÃ¨te pour crÃ©er des assistants IA vraiment utilesâ€¦ et maÃ®trisÃ©s. Venez, il y aura mÃªme une dÃ©mo.

## ğŸ“ Ã€ propos

Cette dÃ©monstration illustre comment crÃ©er un assistant IA intelligent en orchestrant des modÃ¨les locaux et distants avec n8n, incluant :

- ğŸ¤– **Chat assistant** avec streaming en temps rÃ©el
- ğŸ“š **SystÃ¨me RAG** (Retrieval-Augmented Generation) avec Qdrant
- ğŸ”§ **Interface d'administration** pour la gestion des donnÃ©es
- ğŸ‹ **Stack complÃ¨te dockerisÃ©e** pour un dÃ©ploiement facile

## ğŸ—ï¸ Architecture

```mermaid
graph TB
    User[ğŸ‘¤ Utilisateur] --> Chat[ğŸ’¬ Interface Chat]
    Chat --> N8N[ğŸ”§ n8n Orchestrateur]
    N8N --> PG[(ğŸ—„ï¸ PostgreSQL)]
    N8N --> Agent[ğŸ¤– AI Agent]
    Agent --> Ollama[ğŸ§  Ollama LLMs locaux]
    Agent --> Ollama_Remote[â˜ï¸ Ollama LLMs distants]
    Agent --> Qdrant[ğŸ” Qdrant Vector DB]
    Agent --> MCP[ğŸ­ Playwright MCP]
    Agent --> API[ğŸ§  OpenAI/GPT-4]

    subgraph "ğŸ³ Docker Compose Stack"
        Chat
        N8N
        PG
        Ollama
        Qdrant
        MCP
        Agent
    end
    
    style User fill:#e1f5fe
    style Chat fill:#f3e5f5
    style N8N fill:#e8f5e8
    style Agent fill:#fff3e0
    style Ollama fill:#fce4ec
    style API fill:#fce4ec
    style MCP fill:#fce4ec
    style Qdrant fill:#f1f8e9
    style PG fill:#e3f2fd
```

**Composants principaux :**
- **Interface Chat** : Frontend web moderne avec sÃ©lecteur de modÃ¨les (Local/Remote/OpenAI)
- **n8n** : Orchestrateur central gÃ©rant les workflows d'IA
- **AI Agent** : Logique mÃ©tier avec capacitÃ©s RAG et outils MCP
- **ModÃ¨les IA** : Support Ollama local/distant + APIs externes (OpenAI)
- **Playwright MCP** : Web scraping et automatisation navigateur
- **Qdrant** : Base vectorielle pour la recherche sÃ©mantique
- **PostgreSQL** : Persistance des donnÃ©es et sessions

## ğŸš€ DÃ©marrage rapide

### PrÃ©requis

- Linux, macOS ou Windows avec WSL2
- Docker & Docker Compose (ou podman â¤ï¸)
- 8 Go de RAM minimum (recommandÃ©: 16 Go)
- 10 Go d'espace disque libre

### 1. Cloner et configurer

```bash
git clone https://github.com/antoninBr/talk-n8n-agent.git
cd talk-n8n-agent

# Creer un fichier .env Ã  partir de l'exemple
cp .env.example .env

# GÃ©nÃ©rer les clÃ©s et configuration
./generate-key.sh
```

### 2. Lancer la stack complÃ¨te et initialiser

```bash
# DÃ©marrage automatique avec script
./start.sh --import --init-collections --setup-ollama
```

### 3. AccÃ©der aux services

- **ğŸ¨ Chat Assistant** : https://localhost:8443
- **âš™ï¸ Interface n8n** : http://localhost:5678
- **ğŸ“Š Qdrant** : http://localhost:6333
- **ğŸ¦™ Ollama** : http://localhost:11435
- **ğŸ­ Playwright MCP** : http://localhost:3333

### 4. Creer un compte administrateur n8n

- AccÃ©dez Ã  l'interface n8n (http://localhost:5678)
- CrÃ©ez un compte administrateur

### 5. Modifications des credentials OpenAI et Ollama distant (optionnel)

- AccÃ©dez Ã  l'interface n8n (http://localhost:5678)
- Mettez Ã  jour les credentials pour OpenAI et Ollama distant selon vos besoins

### 6. Enregistrer votre instance n8n (optionnel)

- Dans l'interface n8n, allez dans "Settings" > "Instance"
- Cliquez sur "Register Instance" pour enregistrer votre instance n8n et bÃ©nÃ©ficier des mises Ã  jour et du support (gratuit pour les instances auto-hÃ©bergÃ©es)

## ğŸ“ Structure du projet

```
â”œâ”€â”€ ğŸ“± chat_app/              # Interface web du chat assistant
â”œâ”€â”€ ğŸ“Š docs/                  # Slides de prÃ©sentation (Reveal.js)
â”œâ”€â”€ ğŸ—ƒï¸ vector-store-qdrant/   # Configuration Qdrant
â”œâ”€â”€ ğŸ”§ workflows/             # Workflows n8n
â”œâ”€â”€ ğŸ”‘ credentials/           # Credentials n8n (Ollama, Qdrant)
â”œâ”€â”€ ğŸ‹ docker-compose.yml     # Stack complÃ¨te
â””â”€â”€ ğŸ“œ                        # start.sh, stop.sh, clean.sh...
```

## ğŸ¯ FonctionnalitÃ©s dÃ©montrÃ©es

### ğŸ’¬ Assistant Chat
- Streaming en temps rÃ©el
- Gestion de sessions
- Choix du modÃ¨le IA (local/distant/OpenAI)
- Interface moderne et responsive

### ğŸ§  SystÃ¨me RAG
- **Upload de documents** : PDF seulement
- **Web scraping** : Extraction automatique de contenu web
- **Base vectorielle** : Stockage et recherche sÃ©mantique avec Qdrant
- **Collections management** : Interface de gestion des donnÃ©es

### ğŸ”§ Administration
- Monitoring des collections Qdrant
- Gestion des documents indexÃ©s
- Interface d'administration intÃ©grÃ©e

## ğŸ¤ Slides de prÃ©sentation

Les slides sont disponibles dans le dossier `/docs` et utilisent Reveal.js.

**Sujet** : "IA locales ou distantes, outils, et un chef d'orchestre nommÃ© n8n"

**Contenu** :
- Introduction aux agents IA
- Orchestration avec n8n
- DÃ©monstration live du systÃ¨me RAG
- Bonnes pratiques et retours d'expÃ©rience

### Lien des slides live
https://antoninbr.github.io/talk-n8n-agent/

## âš™ï¸ Configuration avancÃ©e

### Variables d'environnement

Copiez `.env.example` vers `.env` et ajustez :

```bash
# Base de donnÃ©es
POSTGRES_DB=n8n
POSTGRES_USER=n8n
POSTGRES_PASSWORD=n8n
```

### ModÃ¨les IA supportÃ©s

- **Local** : Ollama (llama2, mistral, codellama...)
- **Distant** : OpenAI GPT, Anthropic Claude, etc.

## ğŸ› ï¸ Scripts utiles

```bash
# DÃ©marrer la stack la premiÃ¨re fois
./start.sh --import --init-collections --setup-ollama

# DÃ©marrer la stack
./start.sh

# ArrÃªter la stack
./stop.sh

# Nettoyer complÃ¨tement (attention: supprime les donnÃ©es)
./clean.sh

# Creation des collections Qdrant
./setup-collections.sh

# Configurer Ollama avec les modÃ¨les
./setup-ollama.sh

# Importer les workflows n8n
./import-n8n-data.sh
```

## ğŸ“š Documentation dÃ©taillÃ©e

- **Chat App** : Voir [chat_app/README.md](./chat_app/README.md)
- **Workflows n8n** : Documentation dans l'interface n8n
- **Configuration Qdrant** : [vector-store-qdrant/](./vector-store-qdrant/)

## ğŸ› DÃ©pannage

### Services qui ne dÃ©marrent pas
```bash
# VÃ©rifier les logs
docker compose logs

# RedÃ©marrer un service spÃ©cifique
docker compose restart n8n
```

### ProblÃ¨mes de mÃ©moire
- Ollama nÃ©cessite minimum 5 Go de RAM
- Ajustez les modÃ¨les selon votre configuration

### Port dÃ©jÃ  utilisÃ©

```bash
# Changer les ports expoÃ©s sur le l'host dans docker-compose.yml
# Par dÃ©faut: 8080 et 8443 (chat), 3333 (playwright-mcp), 11435 (ollama)
```

Pour le port 5678 (n8n), modifiez la variable d'environnement `N8N_PORT` dans `.env`.

## ğŸ¤ Contribution

Cette dÃ©mo est conÃ§ue pour Ãªtre Ã©ducative et facilement adaptable. N'hÃ©sitez pas Ã  :

- Forker le projet
- Adapter les workflows Ã  vos besoins
- Contribuer aux amÃ©liorations

## ğŸ“„ Licence

MIT License - Voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

---

**PrÃ©sentÃ© Ã  Codeurs en Seine 2025**  
Par [Antonin Brugnot](https://github.com/antoninBr) https://www.youtube.com/watch?v=8UMn9bxHX34


**ğŸ“š Documentation principale :**
- [n8n Documentation](https://docs.n8n.io/) - Documentation officielle de n8n
- [Qdrant Documentation](https://qdrant.tech/documentation/) - Guide complet pour Qdrant Vector Store
- [Ollama Models](https://ollama.ai/library) - BibliothÃ¨que des modÃ¨les Ollama disponibles

**ğŸ¤– IA et modÃ¨les :**
- [Installation Ollama](https://ollama.com/install.sh) - Script d'installation automatique d'Ollama
- [n8n Assistant GPT](https://chatgpt.com/g/g-SVatmGSdQ-n8n-assistant-by-nskha) - Assistant ChatGPT spÃ©cialisÃ© pour n8n

**ğŸ”§ Workflows et ressources :**
- [n8n Workflows Community](https://n8n.io/workflows/) - BibliothÃ¨que communautaire de workflows n8n
- [Reveal.js Documentation](https://revealjs.com/) - Framework de prÃ©sentation utilisÃ© pour les slides

**â˜ï¸ Cloud et dÃ©ploiement :**
- [Docker Ã— Google Cloud Partnership](https://cloud.google.com/blog/products/serverless/cloud-run-and-docker-collaboration) - Partenariat pour simplifier le dÃ©ploiement Cloud Run
