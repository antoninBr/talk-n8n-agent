<!doctype html>
<html lang="en">

<head>
	<meta charset="utf-8">


	<meta name="description" content="IA locales ou distantes, outils, et un chef d'orchestre nommÃ© n8n">
	<meta name="author" content="Antonin Brugnot">

	<meta name="mobile-web-app-capable" content="yes">
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

	<meta name="viewport" content="width=device-width, initial-scale=1.0">

	<title>IA locales ou distantes, outils, et un chef d'orchestre nommÃ© n8n</title>

	<link rel="stylesheet" href="dist/reset.css">
	<link rel="stylesheet" href="dist/reveal.css">
	<link rel="stylesheet" href="dist/theme/codeurs-en-seine.css" id="theme">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="plugin/highlight/monokai.css">
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<!-- Slide d'introduction -->
			<section>
				<div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 2em; padding: 0 2em;">
					<img src="img/codeurs-en-seine-avatar.svg" alt="Codeurs en Seine" 
						style="max-width: 75px; height: auto; padding: 5px; border-radius: 4px;">
					<img src="img/onepoint-logo.svg" alt="Onepoint" 
						style="max-width: 75px; height: auto; background: white; padding: 5px; border-radius: 4px;">
				</div>
				<div style="text-align: center;">
					<img src="img/title.jpg" alt="Logo"
						style="max-width: 150px; width: 20vw; min-width: 100px; margin-bottom: 1em;">
				</div>
				<h3>IA locales ou distantes, outils, et un chef d'orchestre nommÃ© n8n</h3>
				<p>
					<small>Antonin Brugnot - Onepoint</small>
				</p>
				<p style="margin-top: 2em; font-size: 0.8em; color: var(--ces-green);">
					<strong>ğŸ“… 20 Novembre 2025 - Kindarena, Rouen</strong>
				</p>
				<aside class="notes">
					Bonjour ! Aujourd'hui on va explorer comment orchestrer des agents IA avec n8n, une plateforme
					d'automatisation simple et visuelle. On va connecter des modÃ¨les IA locaux avec Ollama ou distants,
					et des outils grÃ¢ce au protocole MCP. Une approche concrÃ¨te pour crÃ©er des assistants IA vraiment
					utilesâ€¦ et maÃ®trisÃ©s.
				</aside>
			</section>

			<!-- Section 1: PrÃ©sentation de n8n -->
			<section>
				<section>
					<h1>ğŸ”§ PrÃ©sentation de n8n</h1>
				</section>

				<section>
					<h2>Pourquoi n8n ?</h2>
					<ul>
						<li class="fragment">ğŸ”— Connecter vos outils du quotidien</li>
						<li class="fragment">ğŸ¨ Interface visuelle intuitive</li>
						<li class="fragment">ğŸš€ No-code/Low-code</li>
						<li class="fragment">ğŸ” ContrÃ´le de vos donnÃ©es</li>
						<li class="fragment">ğŸ’° Open Source</li>
					</ul>
					<aside class="notes">
						n8n permet de connecter facilement tous vos outils : API, bases de donnÃ©es, services cloud.
						L'interface visuelle rend l'automatisation accessible mÃªme aux non-dÃ©veloppeurs. Et
						contrairement Ã  Zapier, vous gardez le contrÃ´le de vos donnÃ©es.
					</aside>
				</section>

				<section>
					<h2>Selfhosted ou Cloud ?</h2>
					<div style="display: flex; justify-content: space-around;">
						<div style="flex: 1;">
							<h3>ğŸ  Self-hosted</h3>
							<ul>
								<li>ContrÃ´le total</li>
								<li>DonnÃ©es privÃ©es</li>
								<li>Personnalisation</li>
								<li>Docker/K8s</li>
							</ul>
						</div>
						<div style="flex: 1;">
							<h3>â˜ï¸ Cloud</h3>
							<ul>
								<li>SimplicitÃ©</li>
								<li>Maintenance incluse</li>
								<li>ScalabilitÃ©</li>
								<li>Support officiel</li>
							</ul>
						</div>
					</div>
					<aside class="notes">
						Deux options : self-hosted pour garder le contrÃ´le total, ou cloud pour la simplicitÃ©.
						Aujourd'hui on se concentre sur le self-hosted avec Docker.
					</aside>
				</section>

				<section>
					<h2>Cas d'usage classiques</h2>
					<ul>
						<li class="fragment">ğŸ“§ Automatisation email</li>
						<li class="fragment">ğŸ”„ Synchronisation de donnÃ©es</li>
						<li class="fragment">ğŸ“Š Rapports automatiques</li>
						<li class="fragment">ğŸ”” Notifications intelligentes</li>
						<li class="fragment">ğŸ› ï¸ Outils internes</li>
					</ul>
					<aside class="notes">
						Les cas d'usage vont de la simple automatisation d'emails Ã  la crÃ©ation d'outils internes
						complexes. Avec l'IA, on peut maintenant crÃ©er des workflows vraiment intelligents.
					</aside>
				</section>

				<section>
					<h2>FonctionnalitÃ©s clÃ©s</h2>
					<ul>
						<li class="fragment">âš¡ Triggers multiples</li>
						<li class="fragment">ğŸ“ Variables entre Ã©tapes</li>
						<li class="fragment">ğŸ§© +400 intÃ©grations</li>
						<li class="fragment">ğŸš¨ Gestion d'erreurs</li>
						<li class="fragment">ğŸ”’ Credentials sÃ©curisÃ©s</li>
					</ul>
					<aside class="notes">
						Les triggers dÃ©clenchent vos workflows : Webhook, Schedule, Email. Les variables permettent de
						passer des donnÃ©es entre Ã©tapes. Plus de 400 nodes disponibles pour connecter tous vos outils.
						Gestion d'erreurs robuste avec retry et fallback. Et stockage sÃ©curisÃ© des credentials.
					</aside>
				</section>

				<section>
					<div style="text-align: center;">
						<img src="img/struggle.jpg" alt="Struggle meme" style="max-width: 80%; margin-bottom: 1em;">
					</div>
				</section>

			</section>

			<!-- Section 2: Avec les agents IA -->
			<section>
				<section>
					<h1>ğŸ¤– Avec les agents IA</h1>
				</section>

				<section>
					<h2>Qu'est-ce qu'un agent IA ?</h2>
					<div style="display: flex; align-items: center; justify-content: space-between; gap: 2em;">
						<div style="flex: 2;">
							<ul>
								<li class="fragment" data-fragment-index="1">ğŸ§  <strong>ModÃ¨le IA</strong> : Le cerveau
									(Krang)</li>
								<li class="fragment" data-fragment-index="2">ğŸ¤– <strong>Agent</strong> : Le corps cyborg
									qui agit</li>
								<li class="fragment" data-fragment-index="3">ğŸ“ <strong>Prompt systÃ¨me</strong> : Les
									instructions de base</li>
								<li class="fragment" data-fragment-index="4">ğŸ› ï¸ <strong>Outils</strong> : Les complices
									(Shredder, Bebop, Rocksteady)</li>
							</ul>
							<p class="fragment" data-fragment-index="5"
								style="font-style: italic; font-size: 0.8em; margin-top: 2em;">
								Agent = ModÃ¨le + Prompt + Outils
							</p>
						</div>
						<div style="flex: 1;" class="fragment" data-fragment-index="1">
							<img src="img/teenage-mutant-ninja-turtles-cartoon.gif" alt="Krang - Le cerveau malÃ©fique"
								style="max-width: 100%; border-radius: 10px; box-shadow: 0 0 20px rgba(255, 0, 255, 0.5);">
							<p style="font-size: 0.6em; font-style: italic; margin-top: 0.5em;">Le cerveau qui orchestre
								tout ! ğŸ§ </p>
						</div>
					</div>
					<aside class="notes">
						Un agent IA, c'est comme dans les Tortues Ninja : Krang est le cerveau (le modÃ¨le IA), le corps
						cyborg c'est l'agent qui peut agir, et ses outils ce sont ses complices Shredder, Bebop et
						Rocksteady qui l'aident Ã  accomplir ses missions !
					</aside>
				</section>

				<section>
					<h2>AI Agent Node</h2>
					<ul>
						<li class="fragment">ğŸ§  Cerveau dÃ©cisionnel</li>
						<li class="fragment">ğŸ’­ Prompts systÃ¨me</li>
						<li class="fragment">ğŸ”— IntÃ©gration native</li>
						<li class="fragment">ğŸ“‹ Planning de tÃ¢ches</li>
					</ul>
					<aside class="notes">
						L'AI Agent Node est le cerveau de votre workflow. Il peut prendre des dÃ©cisions, dÃ©finir son
						comportement via prompts systÃ¨me, s'intÃ©grer nativement aux workflows n8n, et planifier des
						actions complexes en les dÃ©composant selon le contexte.
					</aside>
				</section>
				<section>
					<h2>Node Agent en action</h2>
					<img src="img/agent_node.png" alt="Node Agent n8n connectÃ© Ã  Ollama"
						style="max-width: 80%; border: 2px solid #444;">
					<p style="font-size: 0.8em; margin-top: 1em;">
						<em>Configuration simple : Agent connectÃ© Ã  Ollama local, sans outils ni mÃ©moire</em>
					</p>
					<aside class="notes">
						Voici un exemple concret d'un node Agent dans n8n. Configuration minimaliste : juste le modÃ¨le
						Ollama local, sans outils complexes ni mÃ©moire. Parfait pour commencer et comprendre les bases.
					</aside>
				</section>

				<section>
					<h2>Model Selector</h2>
					<img src="img/model_selector.png" alt="SÃ©lecteur de modÃ¨le dynamique dans n8n"
						style="max-width: 40%; border: 2px solid #444;">
					<ul>
						<li class="fragment">ğŸ¯ Adaptation contextuelle</li>
						<li class="fragment">ğŸ”„ Basculement automatique</li>
						<li class="fragment">ğŸ’¡ Optimisation coÃ»ts/perf</li>
					</ul>
					<aside class="notes">
						Cette fonctionnalitÃ© est trÃ¨s pratique : on peut changer de modÃ¨le dynamiquement selon des
						variables, le type de tÃ¢che, ou implÃ©menter des fallbacks si un modÃ¨le est indisponible. Parfait
						pour optimiser coÃ»ts et performances selon le contexte.
					</aside>
				</section>
				<section>
					<h2>Ollama local vs distant</h2>
					<div style="display: flex; justify-content: space-around;">
						<div style="flex: 1;">
							<h3>ğŸ  Ollama Local</h3>
							<ul>
								<li>ğŸ”’ DonnÃ©es privÃ©es</li>
								<li>âš¡ Pas de latence rÃ©seau</li>
								<li>ğŸ’° Pas de coÃ»t API</li>
								<li>ğŸ–¥ï¸ GPU/CPU local</li>
							</ul>
						</div>
						<div style="flex: 1;">
							<h3>â˜ï¸ ModÃ¨les distants</h3>
							<ul>
								<li>ğŸš€ Performance optimale</li>
								<li>ğŸ”„ Toujours Ã  jour</li>
								<li>ğŸ“ˆ ScalabilitÃ©</li>
								<li>ğŸ’³ Pay-per-use</li>
							</ul>
						</div>
					</div>
					<aside class="notes">
						Ollama permet d'utiliser des modÃ¨les comme Llama, Qwen, ou Mistral en local. Les modÃ¨les
						distants offrent plus de puissance mais moins de contrÃ´le.
					</aside>
				</section>

				<section>
					<div style="text-align: center;">
						<img src="img/girlfriend-meme.jpg" alt="Girlfriend meme about AI models"
							style="max-width: 80%; margin-bottom: 1em;">
					</div>
					<p style="font-size: 0.8em; font-style: italic;">
						"Tu vas encore passer la soirÃ©e avec tes modÃ¨les IA locaux ?"
					</p>
				</section>


				<section>
					<h2>MÃ©moire</h2>
					<ul>
						<li class="fragment">ğŸ§  Conversation multi-tours</li>
						<li class="fragment">ğŸ“š Vector Store (RAG)</li>
						<li class="fragment">ğŸ’¾ Variables persistantes</li>
						<li class="fragment">ğŸ”„ Historique & apprentissage</li>
					</ul>
					<aside class="notes">
						La mÃ©moire permet aux agents de maintenir le contexte dans les conversations, d'accÃ©der Ã  une
						base de connaissances via Vector Store pour le RAG, de persister l'Ã©tat entre workflows, et
						d'apprendre des interactions prÃ©cÃ©dentes grÃ¢ce Ã  l'historique.
					</aside>
				</section>
				<section>
					<h2>Outils disponibles</h2>
					<ul>
						<li class="fragment">ğŸŒ Web & APIs</li>
						<li class="fragment">ğŸ—‚ï¸ Fichiers & Bases de donnÃ©es</li>
						<li class="fragment">ğŸ“§ Communications</li>
						<li class="fragment">ğŸ¤– Autres agents IA</li>
					</ul>
					<p class="fragment" style="margin-top: 2em; font-size: 0.8em;">
						ğŸ’¡ <em>Via MCP ou intÃ©grations natives n8n</em>
					</p>
					<aside class="notes">
						Le protocole MCP (Model Context Protocol) permet aux agents d'utiliser des outils externes :
						navigateur web avec Playwright, fichiers, bases de donnÃ©es SQL/NoSQL/Vectorielles,
						communications Email/Slack/Teams, APIs diverses CRM/ERP/Cloud... et mÃªme d'autres agents IA
						spÃ©cialisÃ©s. J'ai une confÃ©rence complÃ¨te sur MCP pour approfondir :
						github.com/antoninBr/talk-mcp
					</aside>
				</section>

				<section>
					<h2>ğŸ¤–â¡ï¸ğŸ¤–</h2>
					<ul>
						<li class="fragment">ğŸ¯ SpÃ©cialisation par domaine</li>
						<li class="fragment">ğŸ“ DÃ©lÃ©gation intelligente</li>
						<li class="fragment">âš–ï¸ Ã‰quilibrage de charge</li>
						<li class="fragment">ğŸ”„ RÃ©silience & fallback</li>
					</ul>
					<aside class="notes">
						Un agent peut appeler d'autres agents comme outils : un agent gÃ©nÃ©raliste qui dÃ©lÃ¨gue Ã  des
						spÃ©cialistes (code, rÃ©daction, analyse). Cela permet une architecture modulaire et rÃ©siliente en
						microservices, avec Ã©quilibrage de charge et spÃ©cialisation par domaine.
					</aside>
				</section>
				<section>
					<h2>Exemple d'outils</h2>
					<img src="img/tools.png"
						alt="Node Agent avec diffÃ©rents outils : MCP, Call Workflow n8n, Qdrant Vector Store"
						style="max-width: 80%; border: 2px solid #444;">
					<aside class="notes">
						Voici un exemple concret des outils disponibles pour un agent n8n : MCP pour connecter des
						outils externes, Call Workflow pour orchestrer d'autres workflows, et Qdrant Vector Store pour
						la recherche sÃ©mantique. Une palette complÃ¨te pour crÃ©er des agents polyvalents.
					</aside>
				</section>

				<section>
					<h2>Interface Chat : Embedded vs Hosted</h2>
					<img src="img/chat_trigger.png" alt="Trigger Chat : modes embedded et hosted"
						style="max-width: 80%; border: 2px solid #444;">
					<div style="display: flex; justify-content: space-around; margin-top: 1em;">
						<div style="flex: 1;">
							<h4>ğŸ  Embedded</h4>
							<ul style="font-size: 0.8em;">
								<li>Widget intÃ©grÃ©</li>
								<li>Votre design</li>
								<li>ContrÃ´le total</li>
							</ul>
						</div>
						<div style="flex: 1;">
							<h4>â˜ï¸ Hosted</h4>
							<ul style="font-size: 0.8em;">
								<li>Page dÃ©diÃ©e</li>
								<li>PrÃªt Ã  l'emploi</li>
								<li>DÃ©ploiement rapide</li>
							</ul>
						</div>
					</div>
					<aside class="notes">
						n8n propose deux modes d'interaction avec vos agents : embedded pour intÃ©grer le chat dans vos
						applications existantes avec votre propre design, ou hosted pour une page de chat dÃ©diÃ©e et
						prÃªte Ã  l'emploi. Parfait pour tester rapidement ou dÃ©ployer un chatbot interne.
					</aside>
				</section>

				<section>
					<h2>Architecture complÃ¨te</h2>
					<div class="mermaid"><pre>
						graph TB
							User[ğŸ‘¤ Utilisateur] --> Chat[ğŸ’¬ Interface Chat]
							Chat --> N8N[ğŸ”§ n8n]
							N8N --> PG[(ğŸ—„ï¸ PostgreSQL)]
							N8N --> Agent[ğŸ¤– AI Agent]
							Agent --> Ollama[ğŸ§  Ollama LLMs locaux]
							Agent --> Qdrant[ğŸ” Qdrant Vector DB]
							Agent --> MCP[ğŸ­ Playwright MCP]
							Agent --> API[ğŸ§  OpenAI LLMs]

							subgraph "ğŸ³ Docker Compose Stack"
								Chat
								N8N
								PG
								Ollama
								Qdrant
								MCP
								Agent
							end
							
							style User fill:#e1f5fe
							style Chat fill:#f3e5f5
							style N8N fill:#e8f5e8
							style Agent fill:#fff3e0
							style Ollama fill:#fce4ec
							style API fill:#fce4ec
							style MCP fill:#fce4ec
							style Qdrant fill:#f1f8e9
							style PG fill:#e3f2fd
					</pre>
					</div>
					<aside class="notes">
						Architecture locale complÃ¨te avec Docker Compose : interface chat, n8n comme orchestrateur, 
						agent IA connectÃ© Ã  Ollama pour les LLMs et Qdrant pour la recherche vectorielle, 
						le tout persistÃ© en PostgreSQL.
					</aside>
				</section>
			</section>
			<!-- Section 3: DÃ©mo live -->
			<section>
				<section>
					<h1>ğŸš€ DÃ©mo live : n8n sandbox</h1>
				</section>

				<section>
					<h2>Docker compose : stack complet</h2>
					<pre><code class="hljs yaml" data-trim data-line-numbers="|1-43|2-11|13-30|32-39|41-43">
services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: n8n
      POSTGRES_USER: n8n
      POSTGRES_PASSWORD: n8n
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  n8n:
    build: ./n8n
    environment:
      - DB_TYPE=postgresdb
      - DB_POSTGRESDB_HOST=postgres
      - DB_POSTGRESDB_PORT=5432
      - DB_POSTGRESDB_DATABASE=n8n
      - DB_POSTGRESDB_USER=n8n
      - DB_POSTGRESDB_PASSWORD=n8n
      - N8N_HOST=localhost
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
    ports:
      - "5678:5678"
    volumes:
      - n8n_data:/home/node/.n8n
    depends_on:
      - postgres

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11435:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434

volumes:
  postgres_data:
  n8n_data:
  ollama_data:
					</code></pre>
					<aside class="notes">
						Stack complet avec PostgreSQL pour la persistence, n8n comme chef d'orchestre, et Ollama pour
						l'IA locale. Utilisez les flÃ¨ches pour naviguer et voir chaque service mis en focus.
					</aside>
				</section>

				<section>
					<h2>ğŸ³ Docker Model Runner</h2>
					<pre><code class="hljs yaml" data-trim data-line-numbers="|1-19|2-8|10-19">
# Version avec Docker Model Runner (Docker Compose)
services:
  n8n:
    build: ./n8n
    ports:
      - "5678:5678"
    models:
      - llm

models:
  llm:
    model: ollama/llama3.2:3b
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    ports:
      - "11435:11434"
    volumes:
      - ollama_data:/root/.ollama
    runtime: ollama
				</code></pre>
					<aside class="notes">
						Docker Model Runner simplifie la syntaxe pour dÃ©ployer des modÃ¨les IA avec Docker Compose. Plus
						lisible et automatisÃ©, mais encore limitÃ© Ã  Docker Compose. Utilisez les flÃ¨ches pour naviguer :
						vue d'ensemble, service n8n, puis section models. Pour la compatibilitÃ© Podman, on reste sur les
						services classiques.
					</aside>
				</section>

				<section>
					<h2>âœ¨ Avantages</h2>
					<ul>
						<li class="fragment">ğŸ¯ Syntaxe simplifiÃ©e</li>
						<li class="fragment">ğŸ”— Liaison automatique n8n â†” modÃ¨le</li>
						<li class="fragment">âš™ï¸ Configuration centralisÃ©e</li>
						<li class="fragment">ğŸš€ DÃ©ploiement unifiÃ©</li>
					</ul>
					<aside class="notes">
						Les avantages du Docker Model Runner : syntaxe plus claire, liaison automatique entre n8n et les
						modÃ¨les, toute la config au mÃªme endroit, et dÃ©ploiement en une seule commande.
					</aside>
				</section>

				<section>
					<h2>âš ï¸ Limitations</h2>
					<ul>
						<li class="fragment">ğŸ‹ Docker Compose uniquement</li>
						<li class="fragment">ğŸ†• FonctionnalitÃ© rÃ©cente</li>
						<li class="fragment">ğŸ“š Documentation limitÃ©e</li>
						<li class="fragment">ğŸ”§ Fallback classique recommandÃ©</li>
					</ul>
					<aside class="notes">
						Attention aux limitations : pas encore compatible Podman, fonctionnalitÃ© rÃ©cente donc
						potentiellement instable, peu de documentation disponible. Pour la production, prÃ©fÃ©rez la
						configuration classique avec services sÃ©parÃ©s.
					</aside>
				</section>

				<section>
					<h2>Les workflows</h2>
					<ul>
						<li class="fragment">ğŸ“§ <strong>Mail.json</strong> : Assistant email intelligent</li>
						<li class="fragment">ğŸ—‚ï¸ <strong>Indexation.json</strong> : Traitement de documents</li>
						<li class="fragment">ğŸ’¬ <strong>Chat.json</strong> : Chatbot</li>
					</ul>
				</section>

				<section>
					<h2>Demo time! ğŸ¬</h2>
					<h3>DÃ©marrage du stack</h3>
					<pre><code class="hljs bash" data-trim>
						# Lancement des services
						./start.sh

						# Configuration d'Ollama local
						./setup-ollama.sh

						# Import des workflows
						./import-n8n-data.sh
						</code></pre>
					<aside class="notes">
						Maintenant, place Ã  la dÃ©mo ! On va dÃ©marrer le stack, configurer Ollama avec des modÃ¨les
						locaux, et importer nos workflows.
					</aside>
				</section>

				<section data-background="lightblue">
					<h1>ğŸ¯ DÃ©mo en direct</h1>
					<p>http://localhost:5678</p>
					<aside class="notes">
						[Ici, faire la dÃ©monstration live des workflows]
					</aside>
				</section>
			</section>

			<!-- Section Sponsors -->
			<section>
				<section>
					<h1>ğŸ™ Merci Ã  nos sponsors</h1>
					<p style="font-size: 0.8em;">Codeurs en Seine 2025 ne pourrait exister sans le soutien de nos partenaires</p>
				</section>

				<section>
					<h2>Sponsors Platine & Or</h2>
					<div class="sponsors-grid">
						<div class="sponsor-logo">
							<img src="img/sponsors/attineos.png" alt="Attineos">
						</div>
						<div class="sponsor-logo">
							<img src="img/sponsors/bluesoft.png" alt="BlueSoft">
						</div>
						<div class="sponsor-logo">
							<img src="img/sponsors/digit.png" alt="Digit">
						</div>
						<div class="sponsor-logo">
							<img src="img/sponsors/leboncoin.png" alt="Le Bon Coin">
						</div>
						<div class="sponsor-logo">
							<img src="img/sponsors/matmut.png" alt="Matmut">
						</div>
						<div class="sponsor-logo">
							<img src="img/sponsors/sqli.png" alt="SQLI">
						</div>
					</div>
					<aside class="notes">
						Un grand merci Ã  nos sponsors principaux qui rendent cet Ã©vÃ©nement possible : Attineos, BlueSoft, Digit, Le Bon Coin, Matmut et SQLI.
					</aside>
				</section>

				<section>
					<h2>Sponsors Argent & Bronze</h2>
					<div class="sponsors-grid">
						<div class="sponsor-logo">
							<img src="img/sponsors/imagile.png" alt="Imagile">
						</div>
						<div class="sponsor-logo">
							<img src="img/sponsors/limops.png" alt="LimOps">
						</div>
						<div class="sponsor-logo">
							<img src="img/sponsors/magnolia.png" alt="Magnolia">
						</div>
						<div class="sponsor-logo">
							<img src="img/sponsors/proxiad.png" alt="Proxiad">
						</div>
						<div class="sponsor-logo">
							<img src="img/sponsors/redlab.png" alt="REDLab">
						</div>
						<div class="sponsor-logo">
							<img src="img/sponsors/wixiweb.png" alt="Wixiweb">
						</div>
					</div>
					<aside class="notes">
						Merci Ã©galement Ã  nos autres sponsors : Imagile, LimOps, Magnolia, Proxiad, REDLab et Wixiweb.
					</aside>
				</section>

				<section>
					<h2>Partenaires Institutionnels & Ã‰ducatifs</h2>
					<div class="sponsors-grid">
						<div class="sponsor-logo">
							<img src="img/sponsors/metropole-rouen-normandie.png" alt="MÃ©tropole Rouen Normandie">
						</div>
						<div class="sponsor-logo">
							<img src="img/sponsors/cesi.png" alt="CESI">
						</div>
						<div class="sponsor-logo">
							<img src="img/sponsors/nfs.png" alt="Need For School">
						</div>
					</div>
					<aside class="notes">
						Enfin, merci Ã  nos partenaires institutionnels et Ã©ducatifs : la MÃ©tropole Rouen Normandie, le CESI et Need For School. Rendez-vous le 20 novembre 2025 !
					</aside>
				</section>
			</section>

			<!-- Section 4: Conclusion et questions -->
			<section>
				<section>
					<h1>â“ Conclusion et questions</h1>
				</section>

				<section>
					<h2>Ce qu'on a vu</h2>
					<ul>
						<li class="fragment">âœ… n8n comme chef d'orchestre</li>
						<li class="fragment">âœ… IntÃ©gration IA locale (Ollama) et distante</li>
						<li class="fragment">âœ… Agents avec mÃ©moire et outils</li>
						<li class="fragment">âœ… Protocole MCP pour Ã©tendre les capacitÃ©s</li>
						<li class="fragment">âœ… DÃ©mo concrÃ¨te avec workflows</li>
					</ul>
					<aside class="notes">
						RÃ©cap de ce qu'on a couvert : n8n comme plateforme centrale, IA locale et distante, agents
						intelligents avec mÃ©moire et outils, et une dÃ©mo pratique.
					</aside>
				</section>

				<section>
					<h2>Pour aller plus loin</h2>
					<ul>
						<li class="fragment">ğŸ”— <strong>Sources</strong> : GitHub avec docker-compose</li>
						<li class="fragment">ğŸ“š <strong>Documentation</strong> : n8n.io et ollama.ai</li>
						<li class="fragment">ğŸŒ <strong>CommunautÃ©</strong> : Discord n8n et forums</li>
						<li class="fragment">ğŸš€ <strong>Ã‰volutions</strong> : Nouveaux nodes IA en permanence</li>
					</ul>
					<aside class="notes">
						Pour continuer : toutes les sources sont disponibles, la doc officielle est excellente, et la
						communautÃ© trÃ¨s active.
					</aside>
				</section>

				<section>
					<h2 class="r-fit-text">Questions ? ğŸ¤”</h2>
					<p class="fragment">Merci pour votre attention !</p>
				</section>

				<section data-auto-animate>
					<h2 data-id="code-title">bio.yaml</h2>
					<pre data-id="code-animation"><code class="hljs yaml" data-trim data-line-numbers><script type="text/template">
							first_name: "Antonin"
							family_name: "Brugnot"
							company: "Onepoint"
							twitter: null
							personal_info:
							  email: "a.brugnot@groupeonepoint.com"
							  birth: "21st July, 1987"
							  photo: "tronche_joviale.png"
							  location: "Nantes"
							summary: "Lead Tech, DevOps, FullStack, Cloud, IA"
						</script></code></pre>
				</section>

				<section>
					<h2>Contact & Sources</h2>
					<p>ğŸ“§ a.brugnot@groupeonepoint.com</p>
					<p>ğŸ”— GitHub : antoninBr/talk-n8n-agent</p>
					<p>ğŸ¢ Onepoint - Nantes</p>
				</section>
			</section>
		</div>
	</div>

	<script src="dist/reveal.js"></script>
	<script src="plugin/notes/notes.js"></script>
	<script src="plugin/markdown/markdown.js"></script>
	<script src="plugin/highlight/highlight.js"></script>
	<script src="plugin/mermaid/mermaid.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			controls: true,
			progress: true,
			center: true,
			hash: true,
			slideNumber: true,
			// mermaid initialize config
			mermaid: {
				// flowchart: {
				//   curve: 'linear',
				// },
			},
			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealNotes, RevealMarkdown, RevealHighlight, RevealMermaid]
		});
	</script>
</body>

</html>