<!doctype html>
<html lang="en">

<head>
	<meta charset="utf-8">


	<meta name="description" content="IA locales ou distantes, outils, et un chef d'orchestre nomm√© n8n">
	<meta name="author" content="Antonin Brugnot">

	<meta name="mobile-web-app-capable" content="yes">
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

	<meta name="viewport" content="width=device-width, initial-scale=1.0">

	<title>IA locales ou distantes, outils, et un chef d'orchestre nomm√© n8n</title>

	<link rel="stylesheet" href="dist/reset.css">
	<link rel="stylesheet" href="dist/reveal.css">
	<link rel="stylesheet" href="dist/theme/epitech.css" id="theme">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="plugin/highlight/monokai.css">
</head>

<body>
	<div class="reveal">
		<div class="slides">
		<section data-background-image="img/epitech_first_slide.png" data-background-size="contain" data-background-position="center">
		</section>

			<!-- Section 1: Pr√©sentation de n8n -->
			<section>
				<section>
					<h1>üîß Pr√©sentation de n8n</h1>
					<h4>n-odematio-n</h4>
				</section>

				<section>
					<h2>Pourquoi n8n ?</h2>
				<ul style="font-size: 0.85em;">
						<li class="fragment">No-code/Low-code (Workflow visuel)</li>
						<li class="fragment">Contr√¥le de vos donn√©es</li>
						<li class="fragment">Open Source ü•® et en constante √©volution (2.4.6)</li>
					</ul>
					<aside class="notes">
						n8n permet de connecter facilement tous vos outils : API, bases de donn√©es, services cloud.
						L'interface visuelle rend l'automatisation accessible m√™me aux non-d√©veloppeurs. Et
						contrairement √† Zapier, vous gardez le contr√¥le de vos donn√©es.
					</aside>
				</section>

				<section>
					<h2>√Ä quoi ressemble un workflow n8n ?</h2>
					<div style="text-align: center;">
					<img src="img/n8n-workflow-example.png" alt="Exemple de workflow n8n" style="max-width: 55%; border: 2px solid #444; border-radius: 8px;">
				</div>
				<ul style="margin-top: 0.5em; font-size: 0.8em;">
						<li class="fragment"><strong>Trigger</strong> : D√©clencheur (webhook, email, cron...)</li>
						<li class="fragment"><strong>Nodes</strong> : Actions connect√©es visuellement</li>
						<li class="fragment"><strong>Variables</strong> : Donn√©es partag√©es entre √©tapes</li>
						<li class="fragment"><strong>Ex√©cution</strong> : Flux automatis√© de bout en bout</li>
					</ul>
					<aside class="notes">
						Un workflow n8n, c'est une suite d'actions connect√©es visuellement. √áa commence par un trigger 
						(webhook, email, planification), puis des nodes qui s'ex√©cutent en s√©quence ou en parall√®le, 
						avec des variables qui circulent entre les √©tapes. Simple et puissant !
					</aside>
				</section>

				<section>
					<div style="text-align: center;">
						<img src="img/war-vietnam.gif" alt="War meme" style="max-width: 70%; margin-bottom: 1em;">
					</div>
					<p style="font-size: 0.8em; font-style: italic;">
						"Tu connais BPMN ?"
					</p>
				</section>


				<section>
					<h2>Self-hosted ou Cloud ?</h2>
					<div style="display: flex; justify-content: space-around;">
						<div style="flex: 1;">
							<h3>üè† Self-hosted</h3>
							<ul>
								<li>Contr√¥le total</li>
								<li>Donn√©es priv√©es</li>
								<li>Personnalisation</li>
								<li>Docker/K8s</li>
							</ul>
						</div>
						<div style="flex: 1;">
							<h3>‚òÅÔ∏è Cloud</h3>
							<ul>
								<li>Simplicit√©</li>
								<li>Maintenance incluse</li>
								<li>Scalabilit√©</li>
								<li>Support officiel</li>
							</ul>
						</div>
					</div>
					<aside class="notes">
						Deux options : self-hosted pour garder le contr√¥le total, ou cloud pour la simplicit√©.
						Aujourd'hui on se concentre sur le self-hosted avec Docker.
					</aside>
				</section>

				<section>
					<h2>Cas d'usage classiques</h2>
					<ul>
						<li class="fragment">Automatisation email</li>
						<li class="fragment">Synchronisation de donn√©es</li>
						<li class="fragment">Rapports automatiques</li>
						<li class="fragment">Notifications intelligentes (IoT)</li>
						<li class="fragment">Outils internes</li>
					</ul>
					<aside class="notes">
						Les cas d'usage vont de la simple automatisation d'emails √† la cr√©ation d'outils internes
						complexes. Avec l'IA, on peut maintenant cr√©er des workflows vraiment intelligents.
					</aside>
				</section>

				<section>
					<h2>Fonctionnalit√©s cl√©s</h2>
					<ul>
						<li class="fragment">Triggers multiples</li>
						<li class="fragment">Variables entre √©tapes</li>
						<li class="fragment">+400 int√©grations</li>
						<li class="fragment">Gestion d'erreurs</li>
						<li class="fragment">Credentials s√©curis√©s</li>
					</ul>
					<aside class="notes">
						Les triggers d√©clenchent vos workflows : Webhook, Schedule, Email. Les variables permettent de
						passer des donn√©es entre √©tapes. Plus de 400 nodes disponibles pour connecter tous vos outils.
						Gestion d'erreurs robuste avec retry et fallback. Et stockage s√©curis√© des credentials.
					</aside>
				</section>

				<section>
					<h2>Milliers de templates</h2>
					<div style="text-align: center;">
					<img src="img/n8n-workflows.png" alt="Page des workflows n8n" style="max-width: 65%; border: 2px solid #444;">
				</div>
				<p style="margin-top: 0.5em; font-size: 0.9em;">
					üîó <a href="https://n8n.io/workflows/" target="_blank" style="color: var(--ces-green);">n8n.io/workflows/</a>
				</p>
				<p class="fragment" style="font-size: 0.75em; font-style: italic;">
						Pr√™ts √† importer et personnaliser !
					</p>
					<aside class="notes">
						n8n propose des milliers de templates pr√™ts √† l'emploi sur leur site. De l'automatisation email 
						aux int√©grations CRM, en passant par les workflows IA. Un excellent point de d√©part pour vos projets, 
						il suffit d'importer et personnaliser selon vos besoins.
					</aside>
				</section>

				<section>
					<h2>Assistant GPT</h2>
					<div style="text-align: center;">
						<img src="img/n8n-assistant-gpt.png" alt="n8n Assistant GPT" style="max-width: 50%; border: 2px solid #444; border-radius: 8px;">
					</div>
					<p style="margin-top: 1em;">
						üîó <a href="https://chatgpt.com/g/g-SVatmGSdQ-n8n-assistant-by-nskha" target="_blank" style="color: var(--ces-green);">n8n Assistant by nskha</a>
					</p>
					<ul>
						<li class="fragment">Aide √† la cr√©ation de workflows</li>
						<li class="fragment">R√©solution de probl√®mes</li>
						<li class="fragment">Bonnes pratiques</li>
					</ul>
					<aside class="notes">
						Il existe un GPT officiel n8n Assistant cr√©√© par la communaut√© qui peut vous aider dans la cr√©ation 
						de workflows, fournir de la documentation interactive, r√©soudre vos probl√®mes techniques et partager 
						les bonnes pratiques. Un excellent compagnon pour d√©buter avec n8n !
					</aside>
				</section>
				
				<section>
					<div style="text-align: center;">
						<img src="img/struggle.jpg" alt="Struggle meme" style="max-width: 40%; margin-bottom: 1em;">
					</div>
				</section>

			</section>

			<!-- Section 2: Avec les agents IA -->
			<section>
				<section>
					<h1>ü§ñ Avec les agents IA</h1>
				</section>

				<section>
					<h2>Qu'est-ce qu'un agent IA ?</h2>
					<div style="display: flex; align-items: center; justify-content: space-between; gap: 2em;">
						<div style="flex: 2;">
							<ul>
								<li class="fragment" data-fragment-index="1">üß† <strong>Mod√®le IA</strong> : Le cerveau
									(Krang)</li>
								<li class="fragment" data-fragment-index="2">ü§ñ <strong>Agent</strong> : Le corps cyborg
									qui agit</li>
								<li class="fragment" data-fragment-index="3">üìù <strong>Prompt syst√®me</strong> : Les
									instructions de base</li>
								<li class="fragment" data-fragment-index="4">üõ†Ô∏è <strong>Outils</strong> : Les complices
									(Shredder, Bebop, Rocksteady)</li>
							</ul>
							<p class="fragment" data-fragment-index="5"
								style="font-style: italic; font-size: 0.8em; margin-top: 2em;">
								Agent = Mod√®le + Prompt + Outils
							</p>
						</div>
						<div style="flex: 1;" class="fragment" data-fragment-index="1">
							<img src="img/teenage-mutant-ninja-turtles-cartoon.gif" alt="Krang - Le cerveau mal√©fique"
								style="max-width: 100%; border-radius: 10px; box-shadow: 0 0 20px rgba(255, 0, 255, 0.5);">
							<p style="font-size: 0.6em; font-style: italic; margin-top: 0.5em;">Le cerveau qui orchestre
								tout ! üß†</p>
						</div>
					</div>
					<aside class="notes">
						Un agent IA, c'est comme dans les Tortues Ninja : Krang est le cerveau (le mod√®le IA), le corps
						cyborg c'est l'agent qui peut agir, et ses outils ce sont ses complices Shredder, Bebop et
						Rocksteady qui l'aident √† accomplir ses missions !
					</aside>
				</section>

				<section>
					<h2>AI Agent Node</h2>
					<ul>
						<li class="fragment">Cerveau d√©cisionnel</li>
						<li class="fragment">Prompts syst√®me</li>
						<li class="fragment">Int√©gration native</li>
						<li class="fragment">Planning de t√¢ches</li>
					</ul>
					<aside class="notes">
						L'AI Agent Node est le cerveau de votre workflow. Il peut prendre des d√©cisions, d√©finir son
						comportement via prompts syst√®me, s'int√©grer nativement aux workflows n8n, et planifier des
						actions complexes en les d√©composant selon le contexte. Pssssst... c'est du LangChain sous le capot !
					</aside>
				</section>

				<section>
					<h2>Configuration des Prompts</h2>
					<div style="display: flex; justify-content: space-around; align-items: flex-start;">
						<div style="flex: 1; margin-right: 2em;">
							<h3>üéØ Prompt Syst√®me</h3>
							<ul style="font-size: 0.7em;">
								<li class="fragment">D√©finit le comportement</li>
								<li class="fragment">Contexte permanent</li>
								<li class="fragment">R√¥le de l'assistant</li>
								<li class="fragment">Contraintes globales</li>
							</ul>
						</div>
						<div style="flex: 1;">
							<h3>üí¨ Prompt Utilisateur</h3>
							<ul style="font-size: 0.7em;">
								<li class="fragment">Instructions sp√©cifiques</li>
								<li class="fragment">Donn√©es contextuelles</li>
								<li class="fragment">Variables dynamiques</li>
								<li class="fragment">Format de sortie</li>
							</ul>
						</div>
					</div>
					<div class="fragment" style="margin-top: 2em; padding: 1em; background: rgba(0,0,0,0.2); border-radius: 8px;">
						<code style="font-size: 0.7em;">
							System: "Tu es un expert technique qui r√©pond de mani√®re claire et pr√©cise."<br>
							User: "Explique comment {{ $json.technology }} fonctionne en {{ $json.level }} phrases."
						</code>
					</div>
					<aside class="notes">
						Dans n8n, vous configurez deux types de prompts. Le prompt syst√®me d√©finit le comportement g√©n√©ral : 
						le r√¥le, le ton, et les contraintes de l'IA. Le prompt utilisateur contient les instructions sp√©cifiques 
						et peut utiliser des variables dynamiques du workflow. Cette s√©paration permet une grande flexibilit√© 
						et r√©utilisabilit√©.
					</aside>
				</section>

				<section>
					<h2>Int√©gration Mod√®les SaaS</h2>
					<div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 1.3em; margin-top: 0.8em;">
						<div class="fragment" style="text-align: center; padding: 1em; background: rgba(0,0,0,0.2); border-radius: 8px;">
							<h3>OpenAI</h3>
							<p style="font-size: 0.6em;">GPT<br>Streaming, Function calling</p>
						</div>
						<div class="fragment" style="text-align: center; padding: 1em; background: rgba(0,0,0,0.2); border-radius: 8px;">
							<h3>Anthropic</h3>
							<p style="font-size: 0.6em;">Claude<br>Analyse de documents</p>
						</div>
						<div class="fragment" style="text-align: center; padding: 1em; background: rgba(0,0,0,0.2); border-radius: 8px;">
							<h3>Google</h3>
							<p style="font-size: 0.6em;">Gemini Pro<br>Multimodal</p>
						</div>
						<div class="fragment" style="text-align: center; padding: 1em; background: rgba(0,0,0,0.2); border-radius: 8px;">
							<h3>Azure</h3>
							<p style="font-size: 0.6em;">OpenAI sur Azure<br>Conformit√© entreprise</p>
						</div>
					</div>
					<div class="fragment" style="margin-top: 1em; text-align: center;">
						<p style="font-size: 0.6em; color: var(--ces-green);">
							‚úÖ <strong>Configuration simple</strong> : API Key + quelques param√®tres
						</p>
					</div>
					<aside class="notes">
						n8n int√®gre nativement les principaux fournisseurs d'IA. OpenAI pour GPT avec streaming et function calling,
						Anthropic pour Claude excellent sur l'analyse de documents, Google pour Gemini et ses capacit√©s multimodales,
						et Azure OpenAI pour les besoins d'entreprise. La configuration est simple : une cl√© API et quelques param√®tres.
					</aside>
				</section>



				<section>
					<h2>Qu'est-ce qu'Ollama ?</h2>
					<div style="text-align: center; margin-bottom: 2em;">
						<img src="img/serge.jpg" alt="Serge" 
							class="fragment fade-out" data-fragment-index="1"
							style="max-width: 240px; border-radius: 5%; border: 3px solid #444;">
						<img src="img/ollama-logo.png" alt="Ollama Logo" 
							class="fragment fade-in" data-fragment-index="2"
							style="max-width: 90px; background: white; padding: 10px; border-radius: 8px;">
					</div>
					<ul>
						<li class="fragment" data-fragment-index="3"><strong>Docker pour l'IA</strong> : Conteneurise vos LLMs</li>
						<li class="fragment" data-fragment-index="4"><strong>Installation simple</strong> : Une commande</li>
						<li class="fragment" data-fragment-index="5"><strong>Local first</strong> : Vos donn√©es restent chez vous</li>
						<li class="fragment" data-fragment-index="6"><strong>API compatible</strong> : OpenAI-like</li>
					</ul>
					<div class="fragment" data-fragment-index="7" style="margin-top: 0.25em; padding: 0.5em; background: rgba(0,0,0,0.2); border-radius: 8px;">
						<code style="font-size: 0.7em;">
							curl -fsSL https://ollama.com/install.sh | sh ‚ö†Ô∏è<br>
							ollama run llama3.2
						</code>
					</div>
					<aside class="notes">
						Ollama, c'est un peu le Docker de l'IA : il vous permet de t√©l√©charger, installer et faire tourner 
						des mod√®les comme Llama, Mistral ou Qwen en local facilement. Plus besoin de Python complexe, 
						juste deux commandes et vous avez votre IA perso qui tourne !
					</aside>
				</section>

				<section>
					<h2>Int√©gration Ollama</h2>
					<div style="padding: 1em; background: rgba(0,0,0,0.2); border-radius: 8px; margin-bottom: 1em;">
						<h4>üéØ Configuration n8n</h4>
						<code style="font-size: 0.8em;">
							Base URL: http://ollama:11434 (local)<br>
							Base URL: https://your-server.com:11434 (distant)<br>
							Model: llama3.2, qwen2.5, mistral, codestral...
						</code>
					</div>
					<aside class="notes">
						Ollama est parfait pour l'IA locale. Installation simple, donn√©es priv√©es, pas de co√ªts r√©currents.
						Vous pouvez aussi l'installer sur un serveur distant pour partager les ressources GPU avec votre √©quipe.
						Dans n8n, il suffit de configurer l'URL de base et choisir le mod√®le.
					</aside>
				</section>

				<section>
					<h2>Ollama local vs distant</h2>
					<div style="display: flex; justify-content: space-around;">
						<div style="flex: 1;">
							<h3>üè† Ollama Local</h3>
							<ul>
								<li>Donn√©es priv√©es</li>
								<li>Pas de latence r√©seau</li>
								<li>Pas de co√ªt API</li>
								<li>GPU/CPU local</li>
							</ul>
						</div>
						<div style="flex: 1;">
							<h3>‚òÅÔ∏è Mod√®les distants</h3>
							<ul>
								<li>Performance optimale</li>
								<li>Toujours √† jour</li>
								<li>Scalabilit√©</li>
								<li>Pay-per-use</li>
							</ul>
						</div>
					</div>
					<aside class="notes">
						Ollama permet d'utiliser des mod√®les comme Llama, Qwen, ou Mistral en local. Les mod√®les
						distants offrent plus de puissance mais moins de contr√¥le.
					</aside>
				</section>

				<section>
					<div style="text-align: center;">
						<img src="img/drake.jpg" alt="Drake meme about AI models"
							style="max-width: 80%; margin-bottom: 1em;">
					</div>
					<p style="font-size: 0.8em; font-style: italic;">
						"Il faut faire confiance aux mod√®les distants‚Ä¶"
					</p>
				</section>

				<section>
					<h2>Le petit hic d'Ollama</h2>
					<div style="text-align: center;">
						<img src="img/doorstep.gif" alt="Meme about context size"
						style="max-width: 50%; margin-bottom: 0.5em;">
				</div>
				<ul style="font-size: 0.8em;">
						<li class="fragment"><strong>Taille de contexte limit√©e</strong> vs mod√®les SaaS</li>
						<li class="fragment">8K-32K tokens (Ollama) vs 128K-2M tokens (GPT-4)</li>
						<li class="fragment">Impact sur les conversations longues</li>
						<li class="fragment">Trade-off : Contr√¥le vs Performance</li>
					</ul>
					<aside class="notes">
						Le principal inconv√©nient d'Ollama par rapport aux mod√®les SaaS, c'est la taille du contexte. 
						L√† o√π GPT-4 Turbo g√®re 128K tokens et Claude jusqu'√† 2M tokens, les mod√®les Ollama sont 
						g√©n√©ralement limit√©s √† 8K-32K tokens. √áa peut poser probl√®me pour les conversations tr√®s longues 
						ou l'analyse de gros documents. Mais c'est le prix √† payer pour garder le contr√¥le !
					</aside>
				</section>

				<section>
					<h2>Node Agent en action</h2>
					<img src="img/agent_node.png" alt="Node Agent n8n connect√© √† Ollama"
						style="max-width: 80%; border: 2px solid #444;">
					<p style="font-size: 0.8em; margin-top: 1em;">
						<em>Configuration simple : Agent connect√© √† Ollama local, sans outils ni m√©moire</em>
					</p>
					<aside class="notes">
						Voici un exemple concret d'un node Agent dans n8n. Configuration minimaliste : juste le mod√®le
						Ollama local, sans outils complexes ni m√©moire. Parfait pour commencer et comprendre les bases.
					</aside>
				</section>

				<section>
					<h2>Param√®tres Ollama</h2>
					<img src="img/ollama_params.png" alt="Param√®tres Ollama pour customiser et finetuner un mod√®le"
						style="max-width: 70%; border: 2px solid #444; border-radius: 8px;">
					<aside class="notes">
						Ollama dans n8n offre de nombreux param√®tres pour personnaliser le comportement des mod√®les : 
						contr√¥le de la cr√©ativit√© avec temperature/top_p/top_k, gestion du contexte, prompts syst√®me, 
						et m√™me des options de fine-tuning pour adapter le mod√®le √† vos besoins sp√©cifiques.
					</aside>
				</section>

				<section>
					<h2>Model Selector</h2>
					<div style="text-align: center;">
					<img src="img/model_selector.png" alt="S√©lecteur de mod√®le dynamique dans n8n"
						style="max-width: 40%; border: 2px solid #444;">
						</div>
					<ul>
						<li class="fragment">Adaptation contextuelle</li>
						<li class="fragment">Basculement automatique</li>
						<li class="fragment">Optimisation co√ªts/perf</li>
					</ul>
					<aside class="notes">
						Cette fonctionnalit√© est tr√®s pratique : on peut changer de mod√®le dynamiquement selon des
						variables, le type de t√¢che, ou impl√©menter des fallbacks si un mod√®le est indisponible. Parfait
						pour optimiser co√ªts et performances selon le contexte.
					</aside>
				</section>

				<section>
					<h2>M√©moire</h2>
					<ul>
						<li class="fragment">Conversation multi-tours</li>
						<li class="fragment">Vector Store (RAG)</li>
						<li class="fragment">Variables persistantes</li>
						<li class="fragment">Historique & apprentissage</li>
					</ul>
					<aside class="notes">
						La m√©moire permet aux agents de maintenir le contexte dans les conversations, d'acc√©der √† une
						base de connaissances via Vector Store pour le RAG, de persister l'√©tat entre workflows, et
						d'apprendre des interactions pr√©c√©dentes gr√¢ce √† l'historique.
					</aside>
				</section>

				<section>
					<h2>Types de m√©moire disponibles</h2>
					<div style="text-align: center;">
						<img src="img/memory.png" alt="Types de m√©moire dans n8n : Chat Memory, Vector Store Memory, etc." 
							style="max-width: 90%; border: 2px solid #444; border-radius: 8px;">
					</div>
					<aside class="notes">
						Voici les diff√©rents types de m√©moire disponibles dans n8n pour vos agents IA : Chat Memory pour les conversations, Vector Store Memory pour le RAG, et d'autres options selon vos besoins sp√©cifiques.
					</aside>
				</section>

				<section>
					<h2>Outils disponibles</h2>
					<ul>
						<li class="fragment">Web & APIs</li>
						<li class="fragment">Fichiers & Bases de donn√©es</li>
						<li class="fragment">Communications</li>
						<li class="fragment">Autres agents IA</li>
					</ul>
					<p class="fragment" style="margin-top: 2em; font-size: 0.8em;">
						üí° <em>Via MCP ou int√©grations natives n8n</em>
					</p>
					<aside class="notes">
						Le protocole MCP (Model Context Protocol) permet aux agents d'utiliser des outils externes :
						navigateur web avec Playwright, fichiers, bases de donn√©es SQL/NoSQL/Vectorielles,
						communications Email/Slack/Teams, APIs diverses CRM/ERP/Cloud... et m√™me d'autres agents IA
						sp√©cialis√©s. J'ai une conf√©rence compl√®te sur MCP pour approfondir :
						github.com/antoninBr/talk-mcp
					</aside>
				</section>

				<section>
					<h2>MCP (Model Context Protocol)</h2>
					<ul>
						<li class="fragment">Protocole par Anthropic</li>
						<li class="fragment">~1 an d'existence</li>
						<li class="fragment">Standardise les outils pour agents IA</li>
						<li class="fragment">Talk d√©di√© disponible</li>
					</ul>
					<p class="fragment" style="margin-top: 2em; font-size: 0.8em; color: var(--ces-green);">
						üîó <em>github.com/antoninBr/talk-mcp</em>
					</p>
					<aside class="notes">
						MCP est un protocole r√©cent d'Anthropic qui standardise la fa√ßon dont les agents IA utilisent des outils externes. 
						Encore jeune mais tr√®s prometteur pour l'√©cosyst√®me. J'ai un talk complet d√©di√© sur GitHub pour creuser le sujet !
					</aside>
				</section>

				<section>
					<h2>Quelques MCP sympa avec n8n</h2>
					<ul>
						<li class="fragment">playwright-mcp</li>
						<li class="fragment">tavily-mcp-server</li>
						<li class="fragment">atlassian-mcp-server (meme si integration native existe)</li>
						<li class="fragment">Votre propre mcp-server</li>
					</ul>
					<p class="fragment" style="margin-top: 2em; font-size: 1.2em;">
						ü§Ø <em>n8n-mcp</em>
					</p>
					<aside class="notes">
						Oui on peut avoir un n8n-mcp pour que les agents utilisent n8n comme outil !
					</aside>
				</section>

				<section>
					<div style="text-align: center;">
						<img src="img/inception-meme.jpg" alt="Agent n8n utilisant Playwright MCP pour naviguer sur n8n" 
							style="max-width: 40%; border: 2px solid #444; border-radius: 8px;">
					</div>
				</section>

				<section>
					<h2>ü§ñ‚û°Ô∏èü§ñ</h2>
					<ul>
						<li class="fragment">Sp√©cialisation par domaine</li>
						<li class="fragment">D√©l√©gation intelligente</li>
						<li class="fragment">√âquilibrage de charge</li>
						<li class="fragment">R√©silience & fallback</li>
					</ul>
					<aside class="notes">
						Un agent peut appeler d'autres agents comme outils : un agent g√©n√©raliste qui d√©l√®gue √† des
						sp√©cialistes (code, r√©daction, analyse). Cela permet une architecture modulaire et r√©siliente en
						microservices, avec √©quilibrage de charge et sp√©cialisation par domaine.
					</aside>
				</section>

				<section>
					<img src="img/agent_tool_agent.png" alt="Agent utilisant un autre agent comme outil"
						style="max-width: 100%; border: 2px solid #444;">
				</section>
				
				<section>
					<h2>Exemple d'outils</h2>
					<img src="img/tools.png"
						alt="Node Agent avec diff√©rents outils : MCP, Call Workflow n8n, Qdrant Vector Store"
						style="max-width: 80%; border: 2px solid #444;">
					<aside class="notes">
						Voici un exemple concret des outils disponibles pour un agent n8n : MCP pour connecter des
						outils externes, Call Workflow pour orchestrer d'autres workflows, et Qdrant Vector Store pour
						la recherche s√©mantique. Une palette compl√®te pour cr√©er des agents polyvalents.
					</aside>
				</section>

				<section>
					<h2>Interface Chat : Embedded vs Hosted</h2>
					<img src="img/chat_trigger.png" alt="Trigger Chat : modes embedded et hosted"
						style="max-width: 80%; border: 2px solid #444;">
					<div style="display: flex; justify-content: space-around; margin-top: 1em;">
						<div style="flex: 1;">
							<h4>üè† Embedded</h4>
							<ul style="font-size: 0.8em;">
								<li>Widget int√©gr√©</li>
								<li>Votre design</li>
								<li>Contr√¥le total</li>
							</ul>
						</div>
						<div style="flex: 1;">
							<h4>‚òÅÔ∏è Hosted</h4>
							<ul style="font-size: 0.8em;">
								<li>Page d√©di√©e</li>
								<li>Pr√™t √† l'emploi</li>
								<li>D√©ploiement rapide</li>
							</ul>
						</div>
					</div>
					<aside class="notes">
						n8n propose deux modes d'interaction avec vos agents : embedded pour int√©grer le chat dans vos
						applications existantes avec votre propre design, ou hosted pour une page de chat d√©di√©e et
						pr√™te √† l'emploi. Parfait pour tester rapidement ou d√©ployer un chatbot interne.
					</aside>
				</section>

				<section>
					<h2>Securit√©</h2>
						<ul>
							<li>Noeud Guardrails (entr√©e/sortie)</li>
							<li>Pas dans la d√©mo...</li>
						</ul>
				</section>

				<section>
					<h2>Architecture compl√®te</h2>
					<div class="mermaid"><pre>
						graph TB
							User[üë§ Utilisateur] --> Chat[üí¨ Interface Chat]
							Chat --> N8N[üîß n8n]
							N8N --> PG[(üóÑÔ∏è PostgreSQL)]
							N8N --> Agent[ü§ñ AI Agent]
							Agent --> Ollama[üß† Ollama LLMs locaux]
							Agent --> Ollama_Remote[üß† Ollama LLMs distants]
							Agent --> VectorDB[üîç Vector Stores]
							Agent --> MCP[üé≠ MCP Servers]
							Agent --> API[üß† LLMs SaaS]

							subgraph "üê≥ Docker Compose Stack"
								Chat
								N8N
								PG
								Ollama
								VectorDB
								MCP
								Agent
							end
							
							style User fill:#4fc3f7,stroke:#01579b,stroke-width:2px,color:#000
							style Chat fill:#ba68c8,stroke:#4a148c,stroke-width:2px,color:#000
							style N8N fill:#81c784,stroke:#2e7d32,stroke-width:2px,color:#000
							style Agent fill:#ffb74d,stroke:#ef6c00,stroke-width:2px,color:#000
							style Ollama fill:#f06292,stroke:#c2185b,stroke-width:2px,color:#000
							style API fill:#f06292,stroke:#c2185b,stroke-width:2px,color:#000
							style MCP fill:#f06292,stroke:#c2185b,stroke-width:2px,color:#000
							style VectorDB fill:#aed581,stroke:#689f38,stroke-width:2px,color:#000
							style PG fill:#64b5f6,stroke:#1976d2,stroke-width:2px,color:#000
					</pre>
					</div>
					<aside class="notes">
						Architecture locale compl√®te avec Docker Compose : interface chat, n8n comme orchestrateur, 
						agent IA connect√© √† Ollama pour les LLMs locaux, divers Vector Stores pour la recherche vectorielle, 
						et plusieurs MCP Servers selon vos besoins, le tout persist√© en PostgreSQL.
					</aside>
				</section>
			</section>
			<!-- Section 3: D√©mo live -->
			<section>
				<section>
					<h1>üöÄ D√©mo live : n8n sandbox</h1>
				</section>

				<section>
					<h2>Docker compose : stack complet</h2>
					<pre><code class="hljs yaml" data-trim>
services:
  postgres:
    image: postgres:15
    ...
  n8n:
    build: ./n8n
    environment:
      ...
    ...
    depends_on:
      - postgres
  ollama:
    image: ollama/ollama:latest
    ...
					</code></pre>
					<aside class="notes">
						Stack complet avec PostgreSQL pour la persistence, n8n comme chef d'orchestre, et Ollama pour
						l'IA locale. Utilisez les fl√®ches pour naviguer et voir chaque service mis en focus.
					</aside>
				</section>

				<section>
					<h2>üê≥ Docker Model Runner</h2>
					<pre><code class="hljs yaml" data-trim>
# Version avec Docker Model Runner (Docker Compose)
services:
  n8n:
    build: ./n8n
    models:
      - llm
models:
  llm:
    model: ollama/llama3.2:3b
    runtime: ollama
				</code></pre>
					<aside class="notes">
						Docker Model Runner simplifie la syntaxe pour d√©ployer des mod√®les IA avec Docker Compose. Plus
						lisible et automatis√©, mais encore limit√© √† Docker Compose. Utilisez les fl√®ches pour naviguer :
						vue d'ensemble, service n8n, puis section models. Pour la compatibilit√© Podman, on reste sur les
						services classiques.
					</aside>
				</section>

				<section>
					<h2>‚ú® Avantages</h2>
					<ul>
						<li class="fragment">Syntaxe simplifi√©e</li>
						<li class="fragment">Liaison automatique n8n ‚Üî mod√®le</li>
						<li class="fragment">Configuration centralis√©e</li>
						<li class="fragment">D√©ploiement unifi√©</li>
					</ul>
					<aside class="notes">
						Les avantages du Docker Model Runner : syntaxe plus claire, liaison automatique entre n8n et les
						mod√®les, toute la config au m√™me endroit, et d√©ploiement en une seule commande.
					</aside>
				</section>

				<section>
					<h2>‚ö†Ô∏è Limitations</h2>
					<ul>
						<li class="fragment">Docker Compose uniquement</li>
						<div class="fragment" style="text-align: center; margin: 0.5em 0;">
							<img src="img/podman.gif" alt="Podman sad" style="max-width: 200px;">
						</div>
						<li class="fragment">Fonctionnalit√© r√©cente</li>
						<li class="fragment">Documentation limit√©e</li>
						<li class="fragment">Fallback classique recommand√©</li>
					</ul>
					<aside class="notes">
						Attention aux limitations : pas encore compatible Podman, fonctionnalit√© r√©cente donc
						potentiellement instable, peu de documentation disponible. Pour la production, pr√©f√©rez la
						configuration classique avec services s√©par√©s.
					</aside>
				</section>

				<section>
					<h2>L'avenir ? Docker + Google Cloud Run</h2>
					<p class="fragment" style="margin-top: 1em; font-size: 0.8em; color: var(--ces-green);">
						üîó <a href="https://cloud.google.com/blog/products/serverless/cloud-run-and-docker-collaboration" target="_blank">Partenariat Docker √ó Google Cloud</a>
					</p>
					<aside class="notes">
						Docker et Google Cloud s'associent pour simplifier le d√©ploiement vers Cloud Run. 
						Imaginez : votre stack n8n + Ollama d√©ploy√©e en quelques commandes, avec scaling automatique 
						des mod√®les selon la charge, facturation √† l'usage uniquement, et infrastructure totalement manag√©e. 
						L'avenir du d√©ploiement d'agents IA !
					</aside>
				</section>

				<section>
					<h2>Les workflows</h2>
					<ul>
						<li class="fragment">üóÇÔ∏è <strong>Indexation.json</strong> : Traitement de documents</li>
						<li class="fragment">üí¨ <strong>Chat.json</strong> : Chatbot avec RAG</li>
					</ul>
				</section>

				<section>
					<h2>Mon setup Ollama</h2>
				<div style="display: flex; justify-content: space-around; align-items: flex-start; gap: 1.5em;">
					<div style="flex: 1; text-align: center; position: relative; min-height: 350px;">
						<h3 style="font-size: 1em;">ASUS Zenbook</h3>
						<div style="font-size: 1.8em; margin: 0.3em 0;">üñ•Ô∏è</div>
						<ul style="font-size: 0.65em; text-align: left;">
							<li>Ryzen 7</li>
							<li>16 Go RAM</li>
							<li>SSD</li>
							<li>Radeon int√©gr√© üò¢</li>
						</ul>
						<p style="font-size: 0.6em; color: #999; margin-top: 0.5em;">
								<em>Mod√®les l√©gers uniquement</em>
							</p>
							<!-- Mod√®les flottants pour Zenbook -->
							<div class="fragment floating-model" data-fragment-index="1" style="position: absolute; top: -20px; left: 10%; animation: float 3s ease-in-out infinite;">
								<span style="background: rgba(255,193,7,0.8); padding: 0.3em 0.6em; border-radius: 20px; font-size: 0.6em; color: #000; white-space: nowrap; box-shadow: 0 2px 8px rgba(0,0,0,0.2);">nomic-embed-text</span>
							</div>
							<div class="fragment floating-model" data-fragment-index="2" style="position: absolute; top: 60px; right: 5%; animation: float 4s ease-in-out infinite 0.5s;">
								<span style="background: rgba(40,167,69,0.8); padding: 0.3em 0.6em; border-radius: 20px; font-size: 0.6em; color: #fff; white-space: nowrap; box-shadow: 0 2px 8px rgba(0,0,0,0.2);">qwen2.5:3b</span>
							</div>
							<div class="fragment floating-model" data-fragment-index="3" style="position: absolute; bottom: 20px; left: 20%; animation: float 3.5s ease-in-out infinite 1s;">
								<span style="background: rgba(23,162,184,0.8); padding: 0.3em 0.6em; border-radius: 20px; font-size: 0.6em; color: #fff; white-space: nowrap; box-shadow: 0 2px 8px rgba(0,0,0,0.2);">phi3:mini</span>
							</div>
						</div>
					<div style="flex: 1; text-align: center; position: relative; min-height: 350px;">
						<h3 style="font-size: 1em;">Battlestation</h3>
						<img src="img/battlestation.jpg" alt="PC Gaming Setup" 
							style="max-width: 180px; height: 180px; object-fit: cover; border-radius: 8px; margin-bottom: 0.5em;">
						<ul style="font-size: 0.65em; text-align: left;">
								<li>Ryzen 5</li>
								<li>32 Go RAM</li>
								<li>SSD NVMe</li>
								<li>2 * GeForce RTX 2060 (20 Go vRAM)</li>
							</ul>
							<!-- Mod√®les flottants pour Battlestation -->
							<div class="fragment floating-model" data-fragment-index="4" style="position: absolute; top: -10px; left: 15%; animation: float 2.5s ease-in-out infinite;">
								<span style="background: rgba(220,53,69,0.8); padding: 0.3em 0.6em; border-radius: 20px; font-size: 0.6em; color: #fff; white-space: nowrap; box-shadow: 0 2px 8px rgba(0,0,0,0.2);">mistral-nemo:12b</span>
							</div>
							<div class="fragment floating-model" data-fragment-index="5" style="position: absolute; top: 40px; right: 10%; animation: float 3.8s ease-in-out infinite 0.8s;">
								<span style="background: rgba(111,66,193,0.8); padding: 0.3em 0.6em; border-radius: 20px; font-size: 0.6em; color: #fff; white-space: nowrap; box-shadow: 0 2px 8px rgba(0,0,0,0.2);">mistral:7b</span>
							</div>
							<div class="fragment floating-model" data-fragment-index="6" style="position: absolute; top: 80px; left: 5%; animation: float 4.2s ease-in-out infinite 1.2s;">
								<span style="background: rgba(255,87,34,0.8); padding: 0.3em 0.6em; border-radius: 20px; font-size: 0.6em; color: #fff; white-space: nowrap; box-shadow: 0 2px 8px rgba(0,0,0,0.2);">qwen2.5:14b</span>
							</div>
							<div class="fragment floating-model" data-fragment-index="7" style="position: absolute; bottom: 10px; right: 15%; animation: float 3.2s ease-in-out infinite 1.5s;">
								<span style="background: rgba(0,150,136,0.8); padding: 0.3em 0.6em; border-radius: 20px; font-size: 0.6em; color: #fff; white-space: nowrap; box-shadow: 0 2px 8px rgba(0,0,0,0.2);">qwen3:14b</span>
							</div>
							<div class="fragment floating-model" data-fragment-index="8" style="position: absolute; bottom: 40px; left: 25%; animation: float 3.7s ease-in-out infinite 2s;">
								<span style="background: rgba(233,30,99,0.8); padding: 0.3em 0.6em; border-radius: 20px; font-size: 0.6em; color: #fff; white-space: nowrap; box-shadow: 0 2px 8px rgba(0,0,0,0.2);">gpt-oss:20b</span>
							</div>
						</div>
					</div>

					<style>
						@keyframes float {
							0%, 100% { 
								transform: translateY(0px) rotate(0deg); 
								opacity: 0.9;
							}
							25% { 
								transform: translateY(-10px) rotate(2deg); 
								opacity: 1;
							}
							50% { 
								transform: translateY(-5px) rotate(-1deg); 
								opacity: 0.8;
							}
							75% { 
								transform: translateY(-8px) rotate(1deg); 
								opacity: 1;
							}
						}
						.floating-model {
							z-index: 10;
						}
						.floating-model:hover {
							animation-play-state: paused;
							transform: scale(1.1);
							transition: transform 0.2s ease;
						}
					</style>

					<aside class="notes">
						Voici mon setup personnel pour Ollama avec les mod√®les que chaque machine peut faire tourner ! 
						D'un c√¥t√© mon portable ASUS Zenbook limit√© aux petits mod√®les de 1-3B param√®tres comme llama3.2:1b, 
						qwen2.5:3b ou phi3:mini. De l'autre, ma battlestation gaming avec 2 RTX 2060 qui g√®re 
						confortablement les mod√®les locaux !
					</aside>
				</section>

				<section>
					<h2>Demo time! üé¨</h2>
					<h3>D√©marrage de la stack</h3>
					<pre><code class="hljs bash" data-trim>
						# Lancement des services
						./start.sh --import --init-collections --setup-ollama
						</code></pre>
					<aside class="notes">
						Maintenant, place √† la d√©mo ! On va d√©marrer le stack, configurer Ollama avec des mod√®les
						locaux, et importer nos workflows.
					</aside>
				</section>
			</section>

			<!-- Section 4: Conclusion et questions -->
			<section>
				<section>
					<h1>‚ùì Conclusion et questions</h1>
				</section>

				<section>
					<h2>Ce qu'on a vu</h2>
					<ul>
						<li class="fragment">‚úÖ n8n comme chef d'orchestre</li>
						<li class="fragment">‚úÖ Int√©gration IA locale (Ollama) et distante</li>
						<li class="fragment">‚úÖ Agents avec m√©moire et outils</li>
						<li class="fragment">‚úÖ Protocole MCP pour √©tendre les capacit√©s</li>
						<li class="fragment">‚úÖ D√©mo concr√®te avec workflows</li>
					</ul>
					<aside class="notes">
						R√©cap de ce qu'on a couvert : n8n comme plateforme centrale, IA locale et distante, agents
						intelligents avec m√©moire et outils, et une d√©mo pratique.
					</aside>
				</section>

				<section>
					<h2>Pour aller plus loin</h2>
					<ul>
						<li class="fragment">üîó <strong>Sources</strong> : GitHub avec docker-compose</li>
						<li class="fragment">üìö <strong>Documentation</strong> : n8n.io et ollama.ai</li>
						<li class="fragment">üåê <strong>Communaut√©</strong> : Discord n8n et forums</li>
						<li class="fragment">üöÄ <strong>√âvolutions</strong> : Nouveaux nodes IA en permanence</li>
					</ul>
					<aside class="notes">
						Pour continuer : toutes les sources sont disponibles, la doc officielle est excellente, et la
						communaut√© tr√®s active.
					</aside>
				</section>

				<section>
					<h2 class="r-fit-text">Merci EPITECH</h2>
					<h2 class="r-fit-text">Questions ? ü§î</h2>
					<p class="fragment">Merci pour votre attention !</p>
				</section>

				<section data-auto-animate>
					<h2 data-id="code-title">bio.yaml</h2>
					<pre data-id="code-animation"><code class="hljs yaml" data-trim data-line-numbers><script type="text/template">
							first_name: "Antonin"
							family_name: "Brugnot"
							company: "Onepoint"
							twitter: null
							personal_info:
							  email: "a.brugnot@groupeonepoint.com"
							  birth: "21st July, 1987"
							  photo: "tronche_joviale.png"
							  location: "Nantes"
							summary: "Lead Tech, DevOps, FullStack, Cloud, IA"
						</script></code></pre>
				</section>

				<section>
					<h2>Contact & Sources</h2>
					<div style="display: flex; justify-content: space-around; align-items: center; margin: 2em 0;">
						<div style="text-align: center;">
							<img src="img/talk.svg" alt="QR Code Talk Sources" style="max-width: 240px; margin-bottom: 0.2em;">
							<p style="font-size: 0.5em;">Sources & Code</p>
						</div>
					</div>
					<p>üìß a.brugnot@groupeonepoint.com</p>
					<p>üè¢ Onepoint - Nantes</p>
				</section>
			</section>
		</div>
	</div>

	<script src="dist/reveal.js"></script>
	<script src="plugin/notes/notes.js"></script>
	<script src="plugin/markdown/markdown.js"></script>
	<script src="plugin/highlight/highlight.js"></script>
	<script src="plugin/mermaid/mermaid.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			controls: true,
			progress: true,
			center: true,
			hash: true,
			slideNumber: true,
			// mermaid initialize config
			mermaid: {
				// flowchart: {
				//   curve: 'linear',
				// },
			},
			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealNotes, RevealMarkdown, RevealHighlight, RevealMermaid]
		});
	</script>
</body>

</html>